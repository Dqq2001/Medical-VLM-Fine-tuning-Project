# train_grpo.py - Improved Medical VLM Reinforcement Learning (GRPO) Script
# 
# ğŸ¥ åŒ»ç–—è§†è§‰å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ è„šæœ¬ (GRPO) 
#
#
# åŠŸèƒ½ï¼š
# 1. åŠ è½½ SFT åçš„æ¨¡å‹æˆ–åŸºåº§æ¨¡å‹
# 2. å®šä¹‰å¤šç»´åº¦å¥–åŠ±å‡½æ•°ï¼šæ ¼å¼ã€é•¿åº¦ã€æ­¥éª¤ã€å‡†ç¡®ç‡
# 3. æ‰§è¡Œ GRPO è®­ç»ƒ
# 4. ä¿å­˜æœ€ç»ˆæ¨¡å‹

import sys
import os
import re
import torch
import shutil
from unsloth import FastVisionModel, is_bf16_supported
from trl import GRPOTrainer, GRPOConfig
from datasets import load_dataset
from transformers import AutoTokenizer

# =================================================================
# é…ç½®åŒºåŸŸ
# =================================================================
# å°è¯•æŸ¥æ‰¾çš„æ¨¡å‹è·¯å¾„åˆ—è¡¨ (æŒ‰ä¼˜å…ˆçº§)
MODEL_CANDIDATES = [
    "lora_model",  # ä¼˜å…ˆåŠ è½½å½“å‰ç›®å½•ä¸‹çš„ SFT æ¨¡å‹
    "/root/autodl-tmp/models/unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit",
    "/root/autodl-tmp/models/unsloth/Qwen2-VL-7B-Instruct-bnb-4bit",
    "unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit", # è‡ªåŠ¨ä¸‹è½½
]

DATASET_PATH = "./data"
OUTPUT_DIR = "outputs_grpo" # æ¢å¤ä¸ºåŸæ¥çš„ outputs_grpo
MAX_PROMPT_LENGTH = 1024
MAX_COMPLETION_LENGTH = 1024  # ä» 512 å¢åŠ åˆ° 1024ï¼Œé˜²æ­¢ CoT è¢«æˆªæ–­

def get_model_path():
    for path in MODEL_CANDIDATES:
        if os.path.exists(path) or path.startswith("unsloth/"):
            return path
    return MODEL_CANDIDATES[-1] # Fallback to download

def main():
    print("Starting Improved Medical VLM GRPO Training...")
    
    # 1. æ¨¡å‹åŠ è½½ - å°è¯•å¤šä¸ªè·¯å¾„
    model = None
    tokenizer = None
    
    for model_name in MODEL_CANDIDATES:
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ä¸”ä¸å­˜åœ¨ï¼Œè·³è¿‡
        if not model_name.startswith("unsloth/") and not os.path.exists(model_name):
            continue
            
        print(f"ğŸ“¦ Attempting to load model from: {model_name}")
        print("â³ Loading model weights... (This may take 1-2 minutes)")
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¦‚æœæ˜¯åˆ™å¼ºåˆ¶ local_files_only ä»¥é¿å…ç½‘ç»œå¡é¡¿
            is_local = os.path.exists(model_name)
            
            model, tokenizer = FastVisionModel.from_pretrained(
                model_name=model_name,
                load_in_4bit=True,
                device_map="auto",
                use_gradient_checkpointing="unsloth",
                local_files_only=is_local, # æ¢å¤æ­¤å‚æ•°ä»¥åŠ å¿«æœ¬åœ°åŠ è½½
            )
            print(f"âœ… Successfully loaded: {model_name}")
            break
        except Exception as e:
            print(f"âš ï¸ Failed to load {model_name}: {e}")
            continue
    
    if model is None:
        print("âŒ All model candidates failed to load. Exiting.")
        return

    # 2. LoRA é…ç½®
    if hasattr(model, "peft_config") and len(model.peft_config) > 0:
        print(" Model already has LoRA adapters. Enabling training mode...")
        FastVisionModel.for_training(model)
    else:
        print("ğŸ†• Adding new LoRA adapters...")
        model = FastVisionModel.get_peft_model(
            model,
            finetune_vision_layers=False, # é€šå¸¸é”ä½ Vision Tower
            finetune_language_layers=True,
            finetune_attention_modules=True,
            finetune_mlp_modules=True,
            r=16,
            lora_alpha=16,
            lora_dropout=0,
            bias="none",
            use_rslora=False,
        )

    # 3. æ•°æ®å‡†å¤‡
    if not os.path.exists(DATASET_PATH):
        print(f"Dataset path '{DATASET_PATH}' not found!")
        return
        
    print(f"Loading dataset from {DATASET_PATH}...")
    dataset = load_dataset(DATASET_PATH, split="train")
    
    # ç³»ç»Ÿæç¤ºè¯ï¼šå¼ºè°ƒ CoT (Chain of Thought)
    SYSTEM_PROMPT = """You are a professional radiologist. Analyze the given medical image.
Strictly follow this format for your output:

<reasoning>
Write your detailed observation, reasoning logic, and analysis process here.
</reasoning>
<answer>
Write your final diagnostic conclusion here.
</answer>
"""

    def format_data(sample):
        # ç¡®ä¿å›¾ç‰‡å­˜åœ¨
        if 'image' not in sample:
            return None
            
        messages = [
            {
                "role": "system", 
                "content": [{"type": "text", "text": SYSTEM_PROMPT}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": sample['image']},
                    {"type": "text", "text": "Analyze this image."}
                ]
            }
        ]
        return {
            "prompt": messages,
            "ground_truth": sample['caption']
        }

    # å¤„ç†æ•°æ®é›†
    original_len = len(dataset)
    # 1. å…ˆè¿‡æ»¤æ‰æ²¡æœ‰å›¾ç‰‡çš„æ ·æœ¬
    dataset = dataset.filter(lambda x: x.get('image') is not None)
    
    # 2. æ ¼å¼åŒ–
    dataset = dataset.map(format_data, remove_columns=dataset.column_names, num_proc=4)
    print(f"âœ… Dataset loaded. Samples: {len(dataset)} (Original: {original_len})")

    # 4. å¥–åŠ±å‡½æ•°å®šä¹‰
    
    # (A) æ ¼å¼å¥–åŠ±ï¼šä¸¥æ ¼æ£€æŸ¥ XML æ ‡ç­¾
    def xml_format_reward(completions, **kwargs):
        rewards = []
        pattern = r"<reasoning>[\s\S]*?</reasoning>\s*<answer>[\s\S]*?</answer>"
        for completion in completions:
            text = completion[0]["content"] if isinstance(completion, list) else completion
            match = re.search(pattern, text)
            rewards.append(1.0 if match else 0.0)
        return rewards

    # (B) é•¿åº¦å¥–åŠ±ï¼šé¼“åŠ±è¯¦ç»†æ¨ç†
    def length_reward(completions, **kwargs):
        rewards = []
        target_len = 200 # æœŸæœ›çš„æ¨ç†é•¿åº¦å­—ç¬¦æ•°
        for completion in completions:
            text = completion[0]["content"] if isinstance(completion, list) else completion
            reasoning = re.search(r"<reasoning>(.*?)</reasoning>", text, re.DOTALL)
            if reasoning:
                content = reasoning.group(1).strip()
                # ä½¿ç”¨é«˜æ–¯å‡½æ•°å½¢å¼çš„è½¯å¥–åŠ±ï¼Œåœ¨ target_len é™„è¿‘æœ€é«˜
                # æˆ–è€…ç®€å•çš„éçº¿æ€§å¥–åŠ±
                l = len(content)
                if l < 50: rewards.append(-0.5) # å¤ªçŸ­
                elif l > 500: rewards.append(-0.2) # å¤ªé•¿å¯èƒ½å•°å—¦
                else: rewards.append(0.5)
            else:
                rewards.append(0.0)
        return rewards

    # (C) å‡†ç¡®ç‡å¥–åŠ±ï¼šåŸºäºå…³é”®è¯è¦†ç›–
    def accuracy_reward(completions, ground_truth, **kwargs):
        rewards = []
        for completion, truth in zip(completions, ground_truth):
            text = completion[0]["content"] if isinstance(completion, list) else completion
            
            # æå–é¢„æµ‹ç­”æ¡ˆ
            ans_match = re.search(r"<answer>(.*?)</answer>", text, re.DOTALL)
            if ans_match:
                pred = ans_match.group(1).strip().lower()
            else:
                # é™çº§ç­–ç•¥ï¼šå¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå–æœ€åä¸€éƒ¨åˆ†
                pred = text.split("\n")[-1].strip().lower()
            
            truth = truth.lower()
            
            # ç®€å•çš„è¯è¢‹é‡å è®¡ç®—
            def get_tokens(s):
                return set(re.findall(r"\w+", s)) - {"the", "a", "an", "is", "of", "in", "and", "to"}
                
            pred_tokens = get_tokens(pred)
            truth_tokens = get_tokens(truth)
            
            if not truth_tokens:
                rewards.append(0.5) # é˜²æ­¢é™¤é›¶
                continue
                
            overlap = len(pred_tokens & truth_tokens)
            recall = overlap / len(truth_tokens)
            
            # é˜¶æ¢¯å¼å¥–åŠ±
            if recall > 0.8: rewards.append(2.0)
            elif recall > 0.5: rewards.append(1.0)
            elif recall > 0.2: rewards.append(0.5)
            else: rewards.append(0.0)
            
        return rewards

    # 5. è®­ç»ƒå‚æ•°
    training_args = GRPOConfig(
        output_dir=OUTPUT_DIR,
        run_name="grpo_medical_vlm",
        learning_rate=2e-6,           # ä¿å®ˆçš„å­¦ä¹ ç‡
        adam_beta1=0.9,
        adam_beta2=0.99,
        weight_decay=0.1,
        warmup_ratio=0.05,
        lr_scheduler_type="cosine",
        logging_steps=1,
        per_device_train_batch_size=1, # æ˜¾å­˜å—é™æ—¶è®¾ä¸º 1
        gradient_accumulation_steps=8, # å¢åŠ ç´¯ç§¯æ­¥æ•°ä»¥æ¨¡æ‹Ÿå¤§ batch
        num_generations=4,            # Group Size (G)
        max_prompt_length=MAX_PROMPT_LENGTH,
        max_completion_length=MAX_COMPLETION_LENGTH,
        max_steps=10,                 # è®­ç»ƒæ­¥æ•°
        save_steps=25,
        save_total_limit=2,
        report_to="none",             # å…³é—­ wandb é™¤éé…ç½®äº†
        use_vllm=False,               # è®¾ä¸º False ä¿è¯å…¼å®¹æ€§
        bf16=is_bf16_supported(),
        beta=0.01,                    # KL æƒ©ç½šç³»æ•°
    )

    # 6. åˆå§‹åŒ– Trainer
    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        reward_funcs=[xml_format_reward, length_reward, accuracy_reward],
        args=training_args,
        train_dataset=dataset,
    )

    # 7. å¼€å§‹è®­ç»ƒ
    print(" Starting training...")
    trainer.train()
    
    # 8. ä¿å­˜ç»“æœ
    final_output_dir = "grpo_model"
    print(f"Saving final model to {final_output_dir}...")
    model.save_pretrained(final_output_dir)
    tokenizer.save_pretrained(final_output_dir)
    print("Training completed successfully!")

if __name__ == "__main__":
    main()
